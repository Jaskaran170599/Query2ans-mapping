{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as ks\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "ï»¿from __future__ import print_function\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import cntk as C\n",
    "from cntk.io import MinibatchSource, CTFDeserializer, StreamDef, StreamDefs, INFINITELY_REPEAT, FULL_DATA_SWEEP\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7ad02193dafa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;31m# Convert Query,Passage Text Data to CNTK Text Format(CTF) using 50-Dimension Glove word embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[0mTextDataToCTF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainFileName\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"TrainData.ctf\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Train Data conversion is done\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;31m#     TextDataToCTF(validationFileName,\"ValidationData.ctf\",False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-7ad02193dafa>\u001b[0m in \u001b[0;36mTextDataToCTF\u001b[1;34m(inputfile, outputfile, isEvaluation)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0misEvaluation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mfw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"|qfeatures \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mquery_feature_vector\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" |pfeatures \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mpassage_feature_vector\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" |labels \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlabel_str\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mfw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"|qfeatures \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mquery_feature_vector\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" |pfeatures \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mpassage_feature_vector\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"|qid \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#pre_processing\n",
    "\n",
    "#Initialize Global variables \n",
    "GloveEmbeddings = {}\n",
    "max_query_words = 12\n",
    "max_passage_words = 50\n",
    "emb_dim = 50\n",
    "#The following method takes Glove Embedding file and stores all words and their embeddings in a dictionary\n",
    "def loadEmbeddings(embeddingfile):\n",
    "    global GloveEmbeddings,emb_dim\n",
    "\n",
    "    fe = open(embeddingfile,\"r\",encoding=\"utf-8\",errors=\"ignore\")\n",
    "    for line in fe:\n",
    "        tokens= line.strip().split()\n",
    "        word = tokens[0]\n",
    "        vec = tokens[1:]\n",
    "        vec = \" \".join(vec)\n",
    "        GloveEmbeddings[word]=vec\n",
    "    #Add Zerovec, this will be useful to pad zeros, it is better to experiment with padding any non-zero constant values also.\n",
    "    GloveEmbeddings[\"zerovec\"] = \"0.0 \"*emb_dim\n",
    "    fe.close()\n",
    "\n",
    "\n",
    "def TextDataToCTF(inputfile,outputfile,isEvaluation):\n",
    "    global GloveEmbeddings,emb_dim,max_query_words,max_passage_words\n",
    "\n",
    "    f = open(inputfile,\"r\",encoding=\"utf-8\",errors=\"ignore\")  # Format of the file : query_id \\t query \\t passage \\t label \\t passage_id\n",
    "    fw = open(outputfile,\"w\",encoding=\"utf-8\")\n",
    "    for line in f:\n",
    "        tokens = line.strip().lower().split(\"\\t\")\n",
    "        query_id,query,passage,label = tokens[0],tokens[1],tokens[2],tokens[3]\n",
    "\n",
    "        #****Query Processing****\n",
    "        words = re.split('\\W+', query)\n",
    "        words = [x for x in words if x] # to remove empty words \n",
    "        word_count = len(words)\n",
    "        remaining = max_query_words - word_count  \n",
    "        if(remaining>0):\n",
    "            words += [\"zerovec\"]*remaining # Pad zero vecs if the word count is less than max_query_words\n",
    "        words = words[:max_query_words] # trim extra words\n",
    "        #create Query Feature vector \n",
    "        query_feature_vector = \"\"\n",
    "        for word in words:\n",
    "            if(word in GloveEmbeddings):\n",
    "                query_feature_vector += GloveEmbeddings[word]+\" \"\n",
    "            else:\n",
    "                query_feature_vector += GloveEmbeddings[\"zerovec\"]+\" \"  #Add zerovec for OOV terms\n",
    "        query_feature_vector = query_feature_vector.strip() \n",
    "\n",
    "        #***** Passage Processing **********\n",
    "        words = re.split('\\W+', passage)\n",
    "        words = [x for x in words if x] # to remove empty words \n",
    "        word_count = len(words)\n",
    "        remaining = max_passage_words - word_count  \n",
    "        if(remaining>0):\n",
    "            words += [\"zerovec\"]*remaining # Pad zero vecs if the word count is less than max_passage_words\n",
    "        words = words[:max_passage_words] # trim extra words\n",
    "        #create Passage Feature vector \n",
    "        passage_feature_vector = \"\"\n",
    "        for word in words:\n",
    "            if(word in GloveEmbeddings):\n",
    "                passage_feature_vector += GloveEmbeddings[word]+\" \"\n",
    "            else:\n",
    "                passage_feature_vector += GloveEmbeddings[\"zerovec\"]+\" \"  #Add zerovec for OOV terms\n",
    "        passage_feature_vector = passage_feature_vector.strip() \n",
    "\n",
    "        #convert label\n",
    "        label_str = \" 1 0 \" if label==\"0\" else \" 0 1 \" \n",
    "\n",
    "        if(not isEvaluation):\n",
    "            fw.write(\"|qfeatures \"+query_feature_vector+\" |pfeatures \"+passage_feature_vector+\" |labels \"+label_str+\"\\n\")\n",
    "        else:\n",
    "            fw.write(\"|qfeatures \"+query_feature_vector+\" |pfeatures \"+passage_feature_vector+\"|qid \"+str(query_id)+\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    trainFileName = \"data.tsv\"\n",
    "#     validationFileName = \"validationdata.tsv\"\n",
    "    EvaluationFileName = \"eval1_unlabelled.tsv\"\n",
    "\n",
    "    embeddingFileName = \"glove.6B.50d.txt\"\n",
    "\n",
    "    loadEmbeddings(embeddingFileName)    \n",
    "\n",
    "    # Convert Query,Passage Text Data to CNTK Text Format(CTF) using 50-Dimension Glove word embeddings \n",
    "    TextDataToCTF(trainFileName,\"TrainData.ctf\",False)\n",
    "    print(\"Train Data conversion is done\")\n",
    "#     TextDataToCTF(validationFileName,\"ValidationData.ctf\",False)\n",
    "#     print(\"Validation Data conversion is done\")\n",
    "    TextDataToCTF(EvaluationFileName,\"EvaluationData.ctf\",True)\n",
    "    print(\"Evaluation Data conversion is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data\n",
    "\n",
    "#Initialize Global variables\n",
    "validation_query_vectors = []\n",
    "validation_passage_vectors = []\n",
    "validation_labels = []   \n",
    "q_max_words=12\n",
    "p_max_words=50\n",
    "emb_dim=50\n",
    "\n",
    "## The following LoadValidationSet method reads ctf format validation file and creates query, passage feature vectors and also copies labels for each pair.\n",
    "## the created vectors will be useful to find metrics on validation set after training each epoch which will be useful to decide the best model \n",
    "def LoadData(validationfile,batch_size):\n",
    "    f = open(validationfile,'r',encoding=\"utf-8\")\n",
    "    i=0\n",
    "    for line in f:\n",
    "        i+=1\n",
    "        tokens = line.strip().split(\"|\")  \n",
    "        #tokens[0] will be empty token since the line is starting with |\n",
    "        x1 = tokens[1].replace(\"qfeatures\",\"\").strip() #Query Features\n",
    "        x2 = tokens[2].replace(\"pfeatures\",\"\").strip() # Passage Features\n",
    "        y = tokens[3].replace(\"labels\",\"\").strip() # labels\n",
    "        x1 = [float(v) for v in x1.split()]\n",
    "        x2 = [float(v) for v in x2.split()]\n",
    "        y = [int(w) for w in y.split()]        \n",
    "        y = y[1] # label will be at index 1, i.e. if y = \"1 0\" then label=0 else if y=\"0 1\" then label=1\n",
    "\n",
    "        validation_query_vectors.append(x1)\n",
    "        validation_passage_vectors.append(x2)\n",
    "        validation_labels.append(y)\n",
    "        \n",
    "        if i==batch_size:\n",
    "            i=0\n",
    "            yield(validation_passage_vectors,validation_query_vectors,validation_labels)\n",
    "        #print(\"1\")\n",
    "    \n",
    "#     print(\"Validation Vectors are created\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    trainSetFileName = \"TrainData.ctf\"\n",
    "    validationSetFileName = \"ValidationData.ctf\"\n",
    "    testSetFileName = \"EvaluationData.ctf\"\n",
    "    submissionFileName = \"answer.tsv\"\n",
    "   \n",
    "    LoadValidationSet(validationSetFileName)    #Load Validation Query, Passage Vectors from Validation CTF File\n",
    "    model = TrainAndValidate(trainSetFileName) # Training and validation methods    \n",
    "    GetPredictionOnEvalSet(model,testSetFileName,submissionFileName) # Get Predictions on Evaluation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "q_max=12\n",
    "p_max=50\n",
    "emb_size=50\n",
    "nodes=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders\n",
    "tf.reset_default_graph()\n",
    "q_data=tf.placeholder(dtype=tf.float32,shape=(None,q_max,emb_size),name=\"query_data\")\n",
    "p_data=tf.placeholder(dtype=tf.float32,shape=(None,p_max,emb_size),name=\"passage_data\")\n",
    "targets=tf.placeholder(dtype=tf.float32,shape=(None,1),name=\"score\")\n",
    "\n",
    "#query_encoder\n",
    "with tf.variable_scope(\"q_encoder\"):\n",
    "    enc_cell_q_f=tf.nn.rnn_cell.LSTMCell(nodes,activation=tf.nn.tanh)\n",
    "    enc_cell_q_b=tf.nn.rnn_cell.LSTMCell(nodes,activation=tf.nn.tanh)\n",
    "    _,q_states=tf.nn.bidirectional_dynamic_rnn(enc_cell_q_f,enc_cell_q_b,q_data,dtype=tf.float32)    \n",
    "\n",
    "#passage_encoder\n",
    "with tf.variable_scope(\"p_encoder\"):\n",
    "    enc_cell_p_f=tf.nn.rnn_cell.LSTMCell(nodes,activation=tf.nn.tanh)\n",
    "    enc_cell_p_b=tf.nn.rnn_cell.LSTMCell(nodes,activation=tf.nn.tanh)\n",
    "    _,p_states=tf.nn.bidirectional_dynamic_rnn(enc_cell_p_f,enc_cell_p_b,p_data,dtype=tf.float32) \n",
    "\n",
    "#concatenation (None,2048)=>2048 features\n",
    "encoded_data=tf.concat([q_states[0].h,q_states[0].c,q_states[1].h,q_states[1].c,\n",
    "                        p_states[0].h,p_states[0].c,p_states[1].h,p_states[1].c],\n",
    "                       axis=-1)\n",
    "\n",
    "#evaluating\n",
    "with tf.variable_scope(\"eval_ans\"):\n",
    "    l1=tf.keras.layers.Dense(units=2048,activation=\"tanh\",name=\"l1\")(encoded_data)\n",
    "    l2=tf.keras.layers.Dense(units=1024,activation=\"tanh\",name=\"l2\")(l1)\n",
    "    l3=tf.keras.layers.Dense(units=512,activation=\"tanh\",name=\"l3\")(l2)\n",
    "    l4=tf.keras.layers.Dense(units=256,activation=\"tanh\",name=\"l4\")(l3)\n",
    "    l5=tf.keras.layers.Dense(units=128,activation=\"tanh\",name=\"l5\")(l4)\n",
    "    output=tf.keras.layers.Dense(units=1,activation=\"sigmoid\",name=\"output\")(l5)\n",
    "\n",
    "#loss\n",
    "loss=tf.keras.losses.mean_squared_error(y_pred=output,y_true=targets)\n",
    "#optimizer\n",
    "opti=tf.train.AdamOptimizer(name=\"opti\")\n",
    "#step\n",
    "step=opti.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train\n",
    "# epochs=20\n",
    "# batch_size=32\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     k=0\n",
    "#     batch_loss=0\n",
    "\n",
    "#     while k <500:\n",
    "#         batch_loss=0\n",
    "#         batch_logits=0\n",
    "#         acc=0\n",
    "# #         if k+batch_size > data_len:\n",
    "# #             _, batch_loss, batch_logits = sess.run([optimizer, loss, logits],feed_dict={X:data_x[k:],Y:data_y[k:],\n",
    "# #                                                                                         T:data_y[k:]})\n",
    "# #         else:\n",
    "#         data_x,data_y,data_t=get_batch(k,k+batch_size)\n",
    "# #         print(data_x[0],data_y[0],data_t[0])\n",
    "#         _, batch_loss, batch_logits = sess.run([optimizer, loss, logits_h],feed_dict={X:data_x,Y:data_y,T:data_t})\n",
    "                                                                                            \n",
    "#         accuracy = np.mean(batch_logits.argmax(axis=-1) == data_t[k:k+batch_size])\n",
    "\n",
    "#         print('Epoch {:3} Loss: {:>6.3f} Accuracy: {:>6.4f}'.format(i, batch_loss,accuracy))\n",
    "#         k+=batch_size\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'eval_ans/dense_5/Sigmoid:0' shape=(15, 1) dtype=float32>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remaining\n",
    "'''\n",
    "*preprocessing\n",
    "*dropout\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
