{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import wordnet\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "# DOC2VEC\n",
    "import math as m\n",
    "from gensim.models.doc2vec import Doc2Vec,TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def get_tag(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.wordnet.NOUN\n",
    "    \n",
    "def get_lem(text):\n",
    "    t=\"\"\n",
    "    for m in text:\n",
    "        t+=wl.lemmatize(m[0].lower(),get_tag(m[1]))+\" \"\n",
    "    return t.strip()\n",
    "\n",
    "def get_vecs(data,max_epochs,vec_size):\n",
    "    \n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data[0]+data[1])]\n",
    "    \n",
    "    alpha = 0.025\n",
    "    model = Doc2Vec(size=vec_size,\n",
    "                    alpha=alpha, \n",
    "                    min_alpha=0.00025,\n",
    "                    min_count=1,\n",
    "                    dm =1)\n",
    "    model.build_vocab(tagged_data)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train(tagged_data,\n",
    "                    total_examples=model.corpus_count,\n",
    "                    epochs=model.epochs)\n",
    "        # decrease the learning rate\n",
    "        model.alpha -= 0.0002\n",
    "        # fix the learning rate, no decay\n",
    "        model.min_alpha = model.alpha\n",
    "    \n",
    "#     vec=[]\n",
    "\n",
    "#     for i in data[0]:\n",
    "#         vec.append(model.infer_vector(word_tokenize(i)))\n",
    "    \n",
    "#     quer=model.infer_vector(word_tokenize(data[1][0]))\n",
    "#     del model\n",
    "#     print(quer.shape)\n",
    "#     print(model.docvecs[-1].shape)\n",
    "    return model.docvecs\n",
    "    return vec,quer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file=\"./data.tsv\"\n",
    "file=open(train_file,\"r\",encoding=\"utf-8\")\n",
    "num=[\"1\",\"2\",'3','4','5','6','7','8','9','0']\n",
    "labels=[]\n",
    "\n",
    "def batch_nlp(batch_size=1000,vec_size=200,max_epochs=10,nu=2,mode=1,file=file):\n",
    "    batch_size=(batch_size//10)*10\n",
    "    m=0\n",
    "    k=0\n",
    "    query_f=[]\n",
    "    query_ids=[]\n",
    "    passage_f=[]\n",
    "    dataset=[]\n",
    "    labels=[]\n",
    "    z=0\n",
    "    zero=nu\n",
    "    for f in file:\n",
    "        token=f.strip().split(\"\\t\")\n",
    "        query_id,query,passage,label=token[0],token[1],token[2],token[3]\n",
    "        \n",
    "        z+=1\n",
    "        if z==10:\n",
    "          z=0\n",
    "          zero=nu\n",
    "        if ((label=='0') and (zero == 0) and (mode==1)):\n",
    "           continue\n",
    "                     \n",
    "        if label=='1':\n",
    "          zero+=1\n",
    "        zero-=1\n",
    "        m+=1\n",
    "        #Query Preprocessing\n",
    "        query_ids.append(query_id)\n",
    "        if k==0:\n",
    "            for i in string.punctuation:\n",
    "                query=query.replace(i,\"\")\n",
    "            for i in num:\n",
    "                query=query.replace(i,\"\")\n",
    "            query=pos_tag(word_tokenize(query.strip()))\n",
    "            query=get_lem(query)\n",
    "            \n",
    "            query_f.append(query)\n",
    "            \n",
    "            #Passage Preprocessing\n",
    "        for i in string.punctuation:\n",
    "            passage=passage.replace(i,\"\")\n",
    "        for i in num:\n",
    "            passage=passage.replace(i,\"\")\n",
    "        passage=pos_tag(word_tokenize(passage.strip()))\n",
    "        passage=get_lem(passage)\n",
    "        passage_f.append(passage)    \n",
    "        labels.append(float(label))\n",
    "        k+=1\n",
    "        if k==10:\n",
    "            k=0\n",
    "            \n",
    "            vec=get_vecs((passage_f,query_f),max_epochs,vec_size)\n",
    "#             print(q.shape)\n",
    "            del passage_f\n",
    "            del query_f\n",
    "            passage_f=[]\n",
    "            query_f=[]\n",
    "            for i in range(10):\n",
    "                dataset.append((vec[-1],vec[i]))\n",
    "\n",
    "        if m==batch_size:\n",
    "            m=0\n",
    "            \n",
    "            yield(np.array(dataset),np.array(labels),query_ids)\n",
    "            del dataset\n",
    "            del labels\n",
    "            del query_ids\n",
    "            labels=[]\n",
    "            query_ids=[]\n",
    "            dataset=[]\n",
    "            \n",
    "    yield(np.array(dataset),np.array(labels),query_ids)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gives_mod(a):\n",
    "    return ((a**2).sum())**(0.5)\n",
    "\n",
    "def score(a,b):\n",
    "    return abs(m.acos(a.dot(b)/(gives_mod(a)*gives_mod(b)))*(180/m.pi))/360    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n=1000000\n",
    "batch_size=10\n",
    "max_epochs=50\n",
    "vec_size=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data,label,q=next(batch_nlp(100,vec_size,max_epochs,nu=9,mode=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(data,label):\n",
    "    scor=[]\n",
    "    for i in data:\n",
    "        scor.append(score(i[0],i[1]))\n",
    "    scor=np.array(scor)\n",
    "    return((np.argmax(scor)==np.argmax(np.array(label))).sum()/scor.shape[0])*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.array(label)),label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2, 256)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16877709269724986"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=data[5][0]\n",
    "b=data[5][1]\n",
    "score(a,b)#0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-886046d3ca50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"answer.tsv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"w\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch_nlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvec_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEvaluationFileName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-925d2cd9096e>\u001b[0m in \u001b[0;36mbatch_nlp\u001b[1;34m(batch_size, vec_size, max_epochs, nu, mode, file)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mtoken\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mquery_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpassage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mz\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "EvaluationFileName=\"eval2_unlabelled.tsv\"\n",
    "fw = open(\"answer.tsv\",\"w\",encoding=\"utf-8\")\n",
    "i=0\n",
    "for batch in batch_nlp(10,vec_size,max_epochs,nu=9,mode=0,file=EvaluationFileName):\n",
    "    logits=[]\n",
    "    for a,b in batch[0]:\n",
    "        logits.append(score(a,b))\n",
    "    logits=[str(l[0]) for l in logits]\n",
    "    logits=\"\\t\".join(logits)\n",
    "    fw.write(batch[2][0]+\"\\t\"+logits+\"\\n\")\n",
    "    print(i,end=\" \")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
